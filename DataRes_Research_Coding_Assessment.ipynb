{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sjvfsNTAtbO5"
      },
      "outputs": [],
      "source": [
        "#@title Your Info { display-mode: \"form\" }\n",
        "\n",
        "Name = 'Allan Zhang' #@param {type:\"string\"}lla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8tKmOactZMi"
      },
      "source": [
        "# DataRes Research Coding Assessment\n",
        "\n",
        "## Due by 11:59pm, April 6th\n",
        "\n",
        "Thanks for taking the time to apply to the research team at DataRes! This assessment consists of handful of machine learning exercises that use PyTorch. If you aren't familiar with this library, there's lots of helpful documentation [here](https://pytorch.org/docs/stable/index.html), and **We highly recommend reading it thoroughly if you get stuck** to make sure you're passing correct types/dimensions into the arguments. Lots of code has already been written for you, just make sure to complete sections marked \"TODO:\" or \"Your Implementations\". There are a number of challenging problems to work on, so if you have any questions, please feel free to reach out to us at any time! Lastly, please feel free to submit the assessment even if you can't finish it all, we will take the time to look at every application we recieve.\n",
        "\n",
        "Make sure to submit this completed assessment by 11:59pm, April 6. Submissions will be turned in by emailing this notebook as an .ipynb and the .json with your model's results (more on this below) to us.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFiKOp8Yz1w7"
      },
      "source": [
        "### Before you begin\n",
        "\n",
        "Here are some things to keep in mind:\n",
        "\n",
        "* Everything should be done using the PyTorch library\n",
        "* Reading documentation can be very helpful if you're stuck\n",
        "* Currently, the trainings are initialized to default epochs. Colab is about half as fast as Kaggle, so depending on which one you're using, you can change the number of epochs so training doesn't take forever (~10 minutes is good)\n",
        "* Sometimes you may get an error about the GPU/CUDA. If this happens, try changing the variables by adding this line: x = x.to('cuda') and restarting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKyyP0O8tvbW"
      },
      "source": [
        "### Overview\n",
        "\n",
        "In this assessment, you will be working with the [MiniPlaces dataset](https://github.com/CSAILVision/miniplaces), a dataset of scene images (10+ million images) with a wide variety of real-world environments (400+ unique scene categories). The MiniPlaces dataset is a subset of the [Places2 dataset](http://places2.csail.mit.edu/) and contains 100,000 images for training, 10,000 images for validation, and 10,000 images for testing, each of which has been annotated with one of 100 different scene categories. These images are divided into three folders: train, val, and test.\n",
        "\n",
        "Question 1 will use only some categories of this dataset. We've called this new dataset TinyPlaces. Questions 2-4 will use MiniPlaces (all subcategories).\n",
        "\n",
        "You will be completing machine learning exercises in which you will train models using this dataset. These exercises touch on the following topics:\n",
        "\n",
        "\n",
        "*   Linear, Logistic, and Softmax Regression (OPTIONAL)\n",
        "*   Multi-Layer Perceptrons\n",
        "*   Convolutional Neural Networks (CNNs)\n",
        "\n",
        "Don't worry about having to deal with preprocessing the data... we've already written code to do that for you! Just run the cell below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-10-07T01:59:18.175960Z",
          "iopub.status.busy": "2023-10-07T01:59:18.175512Z",
          "iopub.status.idle": "2023-10-07T02:00:14.735842Z",
          "shell.execute_reply": "2023-10-07T02:00:14.734469Z",
          "shell.execute_reply.started": "2023-10-07T01:59:18.175923Z"
        },
        "id": "gyAwMa4aS2iP",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1z3B1GR7UtHZGrqNjUaep6thHwrN3IYSI\n",
            "From (redirected): https://drive.google.com/uc?id=1z3B1GR7UtHZGrqNjUaep6thHwrN3IYSI&confirm=t&uuid=8b50d55c-36c0-47a1-8885-1c7d4a7996b3\n",
            "To: /home/allan/nvim/python/data_res_application/data.tar.gz\n",
            "100%|████████████████████████████████████████| 423M/423M [00:29<00:00, 14.5MB/s]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting tar.gz file:   0%|          | 0.00/526M [00:00<?, ?B/s]/tmp/ipykernel_2979/2822180764.py:17: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extract(member, os.path.join(root_dir, 'data'))\n",
            "Extracting tar.gz file: 100%|██████████| 526M/526M [00:08<00:00, 63.1MB/s] \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "path = './'\n",
        "os.makedirs(os.path.join(path, 'DataRes_Research_Assessment', 'data'), exist_ok=True)\n",
        "root_dir = os.path.join(path, 'DataRes_Research_Assessment')\n",
        "\n",
        "\n",
        "!pip3 install --upgrade gdown --quiet\n",
        "!gdown 1z3B1GR7UtHZGrqNjUaep6thHwrN3IYSI\n",
        "\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "tar = tarfile.open(\"data.tar.gz\", \"r:gz\")\n",
        "total_size = sum(f.size for f in tar.getmembers())\n",
        "with tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Extracting tar.gz file\") as pbar:\n",
        "    for member in tar.getmembers():\n",
        "        tar.extract(member, os.path.join(root_dir, 'data'))\n",
        "        pbar.update(member.size)\n",
        "tar.close()\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def load_subcategories(txt_path):\n",
        "    subcategories = {}\n",
        "    file = open(txt_path, 'r')\n",
        "    lines = file.readlines()\n",
        "    for i, l in enumerate(lines):\n",
        "        info = l.split()\n",
        "        info[0] = info[0][3:]\n",
        "        subcategories.update({info[0]: {'ori_class_id': int(info[1]), 'class_id': i}})\n",
        "\n",
        "    return subcategories\n",
        "\n",
        "\n",
        "from tqdm import trange\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "def select_samples(subcategories, root_dir, split, n_images_per_subcategory):\n",
        "    samples = []\n",
        "    if split == \"train\":\n",
        "        train_dir = os.path.join(root_dir, \"data\", \"images\", \"train\")\n",
        "        for i in subcategories:\n",
        "            child_dir = os.path.join(train_dir, i[0], i)\n",
        "            pics = random.sample(os.listdir(child_dir), n_images_per_subcategory)\n",
        "            for j in pics:\n",
        "                samples.append((cv2.resize(cv2.imread(os.path.join(child_dir, j)), (32,32)).flatten().tolist(), subcategories[i][\"class_id\"]))\n",
        "    elif split == \"val\":\n",
        "        val_dir = os.path.join(root_dir, \"data\", \"images\")\n",
        "        file = open(os.path.join(root_dir, \"data\", \"val.txt\"), 'r')\n",
        "        lines = file.readlines()\n",
        "        val_data = []\n",
        "        for i in lines:\n",
        "            val_data.append(i.split())\n",
        "        random.shuffle(val_data)\n",
        "        for i in subcategories:\n",
        "            old_id = subcategories[i][\"ori_class_id\"]\n",
        "            count = 0\n",
        "            for j in val_data:\n",
        "                if int(j[1]) == old_id:\n",
        "                    samples.append((cv2.resize(cv2.imread(os.path.join(val_dir, j[0])), (32,32)).flatten().tolist(), subcategories[i][\"class_id\"]))\n",
        "                    count += 1\n",
        "                if count >= n_images_per_subcategory:\n",
        "                    break\n",
        "    return samples\n",
        "\n",
        "def create_tinyplaces(samples, binary=True):\n",
        "    data, labels = [], []\n",
        "    for i in samples:\n",
        "        data.append(i[0])\n",
        "        if binary:\n",
        "            if i[1] >= 10:\n",
        "                labels.append(1)\n",
        "            else:\n",
        "                labels.append(0)\n",
        "        else:\n",
        "            labels.append(i[1])\n",
        "    data = np.array(data)\n",
        "    labels = np.array(labels)\n",
        "    dataset = {\"data\": data, \"label\": labels}\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Set the root directory of the dataset\n",
        "root_dir = './DataRes_Research_Assessment/data'\n",
        "\n",
        "# Load the target subcategories and their class IDs\n",
        "subcategories = load_subcategories(os.path.join(root_dir, 'data', 'categories_tinyplaces.txt'))\n",
        "\n",
        "# Select the samples from the train split of the TinyPlaces dataset\n",
        "train_samples = select_samples(subcategories, root_dir, 'train', 500)\n",
        "\n",
        "# Create the TinyPlaces datasets for binary and multiclass classification\n",
        "tinyplaces_binary_train = create_tinyplaces(train_samples, binary=True)\n",
        "tinyplaces_multi_train = create_tinyplaces(train_samples, binary=False)\n",
        "\n",
        "# Select the samples from the val split of the MiniPlaces dataset\n",
        "val_samples = select_samples(subcategories, root_dir, 'val', 50)\n",
        "\n",
        "# Create the TinyPlaces datasets for binary and multiclass classification\n",
        "tinyplaces_binary_val = create_tinyplaces(val_samples, binary=True)\n",
        "tinyplaces_multi_val = create_tinyplaces(val_samples, binary=False)\n",
        "\n",
        "# Save the TinyPlaces datasets to the data directory\n",
        "data_dir = os.path.join(root_dir, 'data')\n",
        "\n",
        "\n",
        "with open(os.path.join(data_dir, 'tinyplaces_binary_train.pkl'), 'wb') as f:\n",
        "    pickle.dump(tinyplaces_binary_train, f)\n",
        "\n",
        "with open(os.path.join(data_dir, 'tinyplaces_multi_train.pkl'), 'wb') as f:\n",
        "    pickle.dump(tinyplaces_multi_train, f)\n",
        "\n",
        "with open(os.path.join(data_dir, 'tinyplaces_binary_val.pkl'), 'wb') as f:\n",
        "    pickle.dump(tinyplaces_binary_val, f)\n",
        "\n",
        "with open(os.path.join(data_dir, 'tinyplaces_multi_val.pkl'), 'wb') as f:\n",
        "    pickle.dump(tinyplaces_multi_val, f)\n",
        "\n",
        "\n",
        "class TinyPlacesDataset(object):\n",
        "    def __init__(self, data_dict):\n",
        "        self.dataset = data_dict\n",
        "        self.num_samples = len(data_dict['data'])\n",
        "\n",
        "    def subsample(self, ratio=0.1, seed=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        nums = random.sample(range(self.num_samples), int(ratio * self.num_samples))\n",
        "        sub_dataset = {'data': self.dataset['data'][nums], 'label': self.dataset['label'][nums]}\n",
        "        subsampled_dataset = TinyPlacesDataset(sub_dataset)\n",
        "\n",
        "        return subsampled_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGBMfhsBt1VP"
      },
      "source": [
        "## GPU\n",
        "\n",
        "You will train your models using the GPU, which enables parallel processing and is vital for training deep learning models.\n",
        "\n",
        "If you're using Colab, go to:\n",
        "Runtime -> Change Runtime Type -> Hardware Accelerator -> GPU\n",
        "\n",
        "If you're using Kaggle, go to: More Settings (the three dots) -> Accelerator -> GPU P100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:00:14.739407Z",
          "iopub.status.busy": "2023-10-07T02:00:14.737784Z",
          "iopub.status.idle": "2023-10-07T02:00:14.749555Z",
          "shell.execute_reply": "2023-10-07T02:00:14.747879Z",
          "shell.execute_reply.started": "2023-10-07T02:00:14.739372Z"
        },
        "id": "LOGvGDrFS2iQ",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda. Good to go!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if device == torch.device('cuda'):\n",
        "    print(f'Using device: {device}. Good to go!')\n",
        "else:\n",
        "    print('Please set GPU via Edit -> Notebook Settings.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:00:14.751617Z",
          "iopub.status.busy": "2023-10-07T02:00:14.751275Z",
          "iopub.status.idle": "2023-10-07T02:00:15.883869Z",
          "shell.execute_reply": "2023-10-07T02:00:15.882719Z",
          "shell.execute_reply.started": "2023-10-07T02:00:14.751586Z"
        },
        "id": "htAi7fg4S2iR",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Sep 23 12:21:50 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 580.82.09              Driver Version: 580.82.09      CUDA Version: 13.0     |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 4070 ...    Off |   00000000:01:00.0  On |                  N/A |\n",
            "| N/A   46C    P4              9W /   80W |    1466MiB /   8188MiB |     36%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A             955      G   /usr/lib/Xorg                            23MiB |\n",
            "|    0   N/A  N/A            1192      G   Hyprland                                241MiB |\n",
            "|    0   N/A  N/A            1313      G   /opt/zen-browser-bin/zen-bin            324MiB |\n",
            "|    0   N/A  N/A            1339      G   Xwayland                                  2MiB |\n",
            "|    0   N/A  N/A            1975      G   kitty                                    19MiB |\n",
            "|    0   N/A  N/A            2005      G   kitty                                    43MiB |\n",
            "|    0   N/A  N/A            2328      G   /opt/visual-studio-code/code            299MiB |\n",
            "|    0   N/A  N/A          800119      G   /usr/lib/slack/slack                     74MiB |\n",
            "|    0   N/A  N/A          825059      G   /opt/discord/Discord                    126MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# This line of code gives you info about GPU\n",
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:00:15.887740Z",
          "iopub.status.busy": "2023-10-07T02:00:15.887129Z",
          "iopub.status.idle": "2023-10-07T02:00:17.057569Z",
          "shell.execute_reply": "2023-10-07T02:00:17.056608Z",
          "shell.execute_reply.started": "2023-10-07T02:00:15.887706Z"
        },
        "id": "-5SQxfd4S2iR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "root_dir = './DataRes_Research_Assessment/data'\n",
        "\n",
        "class TinyPlacesDataset(object):\n",
        "    def __init__(self, data_dict):\n",
        "        self.dataset = data_dict\n",
        "        self.num_samples = len(data_dict['data'])\n",
        "\n",
        "    def subsample(self, ratio=0.1, seed=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        nums = random.sample(range(self.num_samples), int(ratio * self.num_samples))\n",
        "        sub_dataset = {'data': self.dataset['data'][nums], 'label': self.dataset['label'][nums]}\n",
        "        subsampled_dataset = TinyPlacesDataset(sub_dataset)\n",
        "\n",
        "        return subsampled_dataset\n",
        "with open(os.path.join(root_dir, 'data', 'tinyplaces_binary_train.pkl'), 'rb') as f:\n",
        "    binary_train = TinyPlacesDataset(pickle.load(f))\n",
        "with open(os.path.join(root_dir, 'data', 'tinyplaces_binary_val.pkl'), 'rb') as f:\n",
        "    binary_val = TinyPlacesDataset(pickle.load(f))\n",
        "with open(os.path.join(root_dir, 'data', 'tinyplaces_multi_train.pkl'), 'rb') as f:\n",
        "    multi_train = TinyPlacesDataset(pickle.load(f))\n",
        "with open(os.path.join(root_dir, 'data', 'tinyplaces_multi_val.pkl'), 'rb') as f:\n",
        "    multi_val = TinyPlacesDataset(pickle.load(f))\n",
        "\n",
        "# Convert everything from numpy arrays to tensors and move them to the GPU using .cuda()\n",
        "for dataset in [binary_train, binary_val, multi_train, multi_val]:\n",
        "    for k in ['data', 'label']:\n",
        "        dataset.dataset[k] = torch.tensor(dataset.dataset[k]).float().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:00:17.059627Z",
          "iopub.status.busy": "2023-10-07T02:00:17.059039Z",
          "iopub.status.idle": "2023-10-07T02:00:17.064493Z",
          "shell.execute_reply": "2023-10-07T02:00:17.063250Z",
          "shell.execute_reply.started": "2023-10-07T02:00:17.059593Z"
        },
        "id": "oNfvvZQZS2iR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2m2FeWIubFA"
      },
      "source": [
        "## Q1 Regressions (OPTIONAL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHWtYYNxubsA"
      },
      "source": [
        "### Linear Regression (OPTIONAL)\n",
        "\n",
        "Below, you will implement linear regression. You'll need to use PyTorch to initialize the parameters and implement the linear function and mean squared error.\n",
        "\n",
        "If the prediction score > 0.5, we consider the image to be of outdoor category. Otherwise, we consider it to be indoor category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:00:17.066747Z",
          "iopub.status.busy": "2023-10-07T02:00:17.066112Z",
          "iopub.status.idle": "2023-10-07T02:00:17.084621Z",
          "shell.execute_reply": "2023-10-07T02:00:17.083569Z",
          "shell.execute_reply.started": "2023-10-07T02:00:17.066716Z"
        },
        "id": "FmoaFZKIS2iR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class LinearRegression(object):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        \"\"\"\n",
        "          Initialize the weights and biases using zeros distribution\n",
        "\n",
        "          Parameters:\n",
        "              input_size (int): The input size (dimension of feature vectors)\n",
        "              output_size (int): The output size (dimension of output logits)\n",
        "\n",
        "          Returns:\n",
        "              None.\n",
        "        \"\"\"\n",
        "        # Initialize the weights and biases using zeros\n",
        "        # Make sure your tensors keep track of their gradient\n",
        "        # Move the parameters to GPU (cuda)\n",
        "        ################# Your Implementations #################################\n",
        "        self.W = torch.nn.Parameter(torch.zeros(input_size, output_size, device=device, requires_grad=True, dtype=torch.float64))\n",
        "        self.b = torch.nn.Parameter(torch.zeros(output_size, device=device, requires_grad=True, dtype=torch.float64))\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "\n",
        "    def linear(self, x):\n",
        "        ################# Your Implementations #################################\n",
        "        output = x @ self.W + self.b\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # To make the output shape compact.\n",
        "        return self.linear(x).squeeze()\n",
        "\n",
        "    def get_loss(self, pred_logits, targets):\n",
        "        # Calculate the mean squared error between the predicted labels and the ground-truth labels\n",
        "        loss = None\n",
        "        ################# Your Implementations #################################\n",
        "        loss = torch.mean((pred_logits - targets)**2)\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return loss\n",
        "\n",
        "    def fit(self, x, y, x_val, y_val, lr, epochs=500000, print_freq=1000):\n",
        "        # Fit the linear regression model to the training data using gradient descent\n",
        "        # x is the input data, y is the ground-truth labels\n",
        "        # lr is the learning rate, epochs is the number of epochs\n",
        "\n",
        "        # To store validation accuracy\n",
        "        val_accs = []\n",
        "        # Create a progress bar using tqdm\n",
        "        pbar = tqdm(range(epochs))\n",
        "        for epoch in pbar:\n",
        "            # Calculate the loss\n",
        "            y_pred_logits = self.forward(x)\n",
        "            loss = self.get_loss(y_pred_logits, y)\n",
        "            # Backpropagate the loss to compute the gradients\n",
        "            loss.backward()\n",
        "            # Update the weights and biases using gradient descent\n",
        "            with torch.no_grad():\n",
        "                self.W -= lr * self.W.grad\n",
        "                self.b -= lr * self.b.grad\n",
        "                # Reset the gradients\n",
        "                self.W.grad.zero_()\n",
        "                self.b.grad.zero_()\n",
        "\n",
        "            if epoch % print_freq == 0:\n",
        "                # Calculate the validation accuracy\n",
        "                val_acc = self.evaluate(x_val, y_val)\n",
        "                val_accs.append(val_acc)\n",
        "                # Update the progress bar with the validation accuracy and training loss\n",
        "                pbar.set_description(f'val_acc: {val_acc:.3f}')\n",
        "        return val_accs\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        # Evaluate the performance of the linear regression model on the dataset\n",
        "        # x is the input data, y is the ground-truth labels\n",
        "        # Calculate the predicted labels\n",
        "        y_pred = self.forward(x) > 0.5\n",
        "        return (y_pred == y).float().mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:00:17.087316Z",
          "iopub.status.busy": "2023-10-07T02:00:17.086140Z",
          "iopub.status.idle": "2023-10-07T02:00:17.099650Z",
          "shell.execute_reply": "2023-10-07T02:00:17.098688Z",
          "shell.execute_reply.started": "2023-10-07T02:00:17.087283Z"
        },
        "id": "Mizc7tkcS2iR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def normalize(x):\n",
        "    # We can simply divide x by 255 since its range is (0,255)\n",
        "    return x / 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:00:17.101719Z",
          "iopub.status.busy": "2023-10-07T02:00:17.101135Z",
          "iopub.status.idle": "2023-10-07T02:00:17.112242Z",
          "shell.execute_reply": "2023-10-07T02:00:17.111324Z",
          "shell.execute_reply.started": "2023-10-07T02:00:17.101685Z"
        },
        "id": "1G_UZ_zwS2iR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Here \"ori\" indicates \"original\"\n",
        "X_train_ori, y_train = binary_train.dataset['data'], binary_train.dataset['label']\n",
        "X_val_ori, y_val = binary_val.dataset['data'], binary_val.dataset['label']\n",
        "\n",
        "# Normalization\n",
        "X_train = normalize(X_train_ori)\n",
        "X_val = normalize(X_val_ori)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW0B-W58uilG"
      },
      "source": [
        "Use the next 3 cells to debug. The next cell should give an accuracy of 0.5 since we're using zeros-initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:00:17.114789Z",
          "iopub.status.busy": "2023-10-07T02:00:17.113594Z",
          "iopub.status.idle": "2023-10-07T02:00:17.126758Z",
          "shell.execute_reply": "2023-10-07T02:00:17.125666Z",
          "shell.execute_reply.started": "2023-10-07T02:00:17.114755Z"
        },
        "id": "J_l1H9tYS2iS",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train accuracy: 0.5\n",
            "val accuracy: 0.5\n"
          ]
        }
      ],
      "source": [
        "linear_model = LinearRegression(3072, 1)\n",
        "train_acc = linear_model.evaluate(X_train.double(), y_train)\n",
        "val_acc = linear_model.evaluate(X_val.double(), y_val)\n",
        "print('train accuracy:', train_acc)\n",
        "print('val accuracy:', val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:00:17.131253Z",
          "iopub.status.busy": "2023-10-07T02:00:17.130992Z",
          "iopub.status.idle": "2023-10-07T02:00:17.138773Z",
          "shell.execute_reply": "2023-10-07T02:00:17.137551Z",
          "shell.execute_reply.started": "2023-10-07T02:00:17.131230Z"
        },
        "id": "BsgSuKsoS2iS",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 0.5\n"
          ]
        }
      ],
      "source": [
        "# Try to debug using this cell\n",
        "linear_model = LinearRegression(3072, 1)\n",
        "# We refer the raw outputs from a model to as \"logits\",\n",
        "# i.e., we haven't transformed the results to binary labels.\n",
        "y_pred_logits = linear_model.forward(X_train.double())\n",
        "loss = linear_model.get_loss(y_pred_logits, y_train)\n",
        "print(\"loss:\", loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:00:17.141045Z",
          "iopub.status.busy": "2023-10-07T02:00:17.140225Z",
          "iopub.status.idle": "2023-10-07T02:00:17.155942Z",
          "shell.execute_reply": "2023-10-07T02:00:17.155040Z",
          "shell.execute_reply.started": "2023-10-07T02:00:17.141012Z"
        },
        "id": "GIOgOnd5S2iS",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.5329],\n",
            "        [-0.5047],\n",
            "        [-0.4560],\n",
            "        ...,\n",
            "        [-0.2999],\n",
            "        [-0.3500],\n",
            "        [-0.3588]], device='cuda:0', dtype=torch.float64)\n",
            "tensor([-1.0000], device='cuda:0', dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# compute gradients\n",
        "loss.backward()\n",
        "\n",
        "# check the gradients\n",
        "print(linear_model.W.grad)\n",
        "print(linear_model.b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUFTLarxulrd"
      },
      "source": [
        "Great, you've fully implemented the linear regression model! Let's train it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:00:17.157929Z",
          "iopub.status.busy": "2023-10-07T02:00:17.157393Z",
          "iopub.status.idle": "2023-10-07T02:10:28.302720Z",
          "shell.execute_reply": "2023-10-07T02:10:28.301836Z",
          "shell.execute_reply.started": "2023-10-07T02:00:17.157898Z"
        },
        "id": "W3VjCJZuS2iS",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "val_acc: 0.500: 100%|██████████| 500/500 [00:01<00:00, 391.03it/s]\n"
          ]
        }
      ],
      "source": [
        "linear_model = LinearRegression(3072, 1)\n",
        "# It may be helpful to change the number of epochs to a very small number while debugging\n",
        "# For your final submission, you can change the epochs so that training doesn't take forever (~10 minutes is good)\n",
        "#lin_val_accs = linear_model.fit(X_train.double().cuda(), y_train, X_val.double().cuda(), y_val, 1e-4, 500000, 1000)\n",
        "lin_val_accs = linear_model.fit(X_train.double().cuda(), y_train, X_val.double().cuda(), y_val, 1e-4, 500, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-Ax-AdvuqKT"
      },
      "source": [
        "### Logistic Regression (OPTIONAL)\n",
        "\n",
        "Logistic regression is very similar to linear regression, but the sigmoid function is added to the forward pass and cross-entropy is used for loss. Here's the formulas:\n",
        "\n",
        "Sigmoid:\n",
        "\n",
        "$$\\frac{1}{1+e^{-x}}$$\n",
        "\n",
        "Cross-Entropy Loss:\n",
        "\n",
        "$$\\mathrm{CE_Loss}(p, y) = -{(y\\log(p) + (1-y)\\log(1-p))}$$\n",
        "\n",
        "Implement them below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:10:28.304706Z",
          "iopub.status.busy": "2023-10-07T02:10:28.304155Z",
          "iopub.status.idle": "2023-10-07T02:10:28.310498Z",
          "shell.execute_reply": "2023-10-07T02:10:28.309671Z",
          "shell.execute_reply.started": "2023-10-07T02:10:28.304673Z"
        },
        "id": "GfIt43g3S2iS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    ################# Your Implementations #################################\n",
        "    output = torch.sigmoid(x)\n",
        "\n",
        "    ################# End of your Implementations ##########################\n",
        "    return output\n",
        "\n",
        "\n",
        "def cross_entropy_loss(p, y):\n",
        "    ################# Your Implementations #################################\n",
        "    #output = -(y * torch.log(p)) + (1- y) * torch.log(1 - p)\n",
        "    output = - (y * torch.log(p) + (1 - y) * torch.log(1 - p))\n",
        "    output = torch.mean(output)\n",
        "\n",
        "    ################# End of your Implementations ##########################\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLA5YsCIusxo"
      },
      "source": [
        "Use these implementations below to implement the Logistic Regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:10:28.312991Z",
          "iopub.status.busy": "2023-10-07T02:10:28.311938Z",
          "iopub.status.idle": "2023-10-07T02:10:28.323365Z",
          "shell.execute_reply": "2023-10-07T02:10:28.322530Z",
          "shell.execute_reply.started": "2023-10-07T02:10:28.312934Z"
        },
        "id": "Wzjx1WhRS2iS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(LinearRegression):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LogisticRegression, self).__init__(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the sigmoid function to the linear output\n",
        "        ################# Your Implementations #################################\n",
        "        output = self.linear(x)\n",
        "        output = sigmoid(output)\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return output\n",
        "\n",
        "    def get_loss(self, pred_logits, targets):\n",
        "        # Calculate the cross-entropy loss\n",
        "        ################# Your Implementations #################################\n",
        "        loss = cross_entropy_loss(pred_logits, targets)\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmTKJwhouusX"
      },
      "source": [
        "Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:10:28.324993Z",
          "iopub.status.busy": "2023-10-07T02:10:28.324510Z",
          "iopub.status.idle": "2023-10-07T02:20:41.868366Z",
          "shell.execute_reply": "2023-10-07T02:20:41.867393Z",
          "shell.execute_reply.started": "2023-10-07T02:10:28.324964Z"
        },
        "id": "VAVLfWMgS2iS",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "val_acc: 0.500: 100%|██████████| 50/50 [00:01<00:00, 27.23it/s] \n"
          ]
        }
      ],
      "source": [
        "logistic_model = LogisticRegression(3072, 1)\n",
        "# Again, you can change the epochs so that training doesn't take forever (~10 minutes is good)\n",
        "#logi_val_accs = logistic_model.fit(X_train.double(), y_train, X_val.double(), y_val, 1e-4, 25000, 1000)\n",
        "logi_val_accs = logistic_model.fit(X_train.double(), y_train, X_val.double(), y_val, 1e-4, 50, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aXCtUYtuw3_"
      },
      "source": [
        "Plot the validation accuracies of both models. Logistic Regression should be more accurate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:20:41.870452Z",
          "iopub.status.busy": "2023-10-07T02:20:41.869823Z",
          "iopub.status.idle": "2023-10-07T02:20:42.112235Z",
          "shell.execute_reply": "2023-10-07T02:20:42.111301Z",
          "shell.execute_reply.started": "2023-10-07T02:20:41.870406Z"
        },
        "id": "vW1a48IqS2iT",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(lin_val_accs, label='LinearRegression')\n",
        "plt.plot(logi_val_accs, label='LogisticRegression')\n",
        "plt.legend()\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Validation Accuracy (%)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdxpWU9iu0UV"
      },
      "source": [
        "### Softmax Regression (OPTIONAL)\n",
        "\n",
        "Softmax regression is similar to logistic regression except it uses the softmax function instead of sigmoid and uses negative log likelihood for the loss. Here's the formulas:\n",
        "\n",
        "Softmax:\n",
        "\n",
        "$$\\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ for\\ i=1,2,\\dots,K$$\n",
        "\n",
        "where z is the output of the linear model, and K is the number of classes.\n",
        "\n",
        "Negative Log Likelihood:\n",
        "\n",
        "$$-{\\log(p(y))}$$\n",
        "\n",
        "where p(y) is the predicted probability of the data point belonging to the true class y.\n",
        "\n",
        "Implement them below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:20:42.114271Z",
          "iopub.status.busy": "2023-10-07T02:20:42.113454Z",
          "iopub.status.idle": "2023-10-07T02:20:42.122410Z",
          "shell.execute_reply": "2023-10-07T02:20:42.121295Z",
          "shell.execute_reply.started": "2023-10-07T02:20:42.114239Z"
        },
        "id": "mUqySvfdS2iT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    # Implement the softmax function\n",
        "    # x is the input data with shape (batch_size, input_size)\n",
        "    # Return the softmax output with shape (batch_size, output_size)\n",
        "    ################# Your Implementations #################################\n",
        "    output = torch.softmax(x, dim=1)\n",
        "\n",
        "    ################# End of your Implementations ##########################\n",
        "    return output\n",
        "\n",
        "def nll_loss(pred_probs, targets):\n",
        "    # Calculate the negative log likelihood loss\n",
        "    # pred_probs is the predicted probability distribution with shape (batch_size, output_size)\n",
        "    # targets is the ground-truth labels with shape (batch_size,)\n",
        "    # Return the negative log likelihood loss with shape (batch_size,)\n",
        "\n",
        "    # There are actually two functions for negative log likelihood loss: torch.nn.NLLLoss and torch.nn.functional.nll_loss\n",
        "    # You can convert the ground-truth labels to one-hot encoding using torch.eye() and pass to NLLLoss\n",
        "    # Alternatively, try converting targets to type LongTensor and passing that to functional.nll_loss\n",
        "    ################# Your Implementations #################################\n",
        "    log_probs = torch.log(pred_probs)\n",
        "    loss = torch.nn.functional.nll_loss(log_probs, targets.long())\n",
        "\n",
        "    ################# End of your Implementations ##########################\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En2_dN61u4GY"
      },
      "source": [
        "Use these implementations below to implement the Softmax Regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:20:42.124278Z",
          "iopub.status.busy": "2023-10-07T02:20:42.123678Z",
          "iopub.status.idle": "2023-10-07T02:20:42.137001Z",
          "shell.execute_reply": "2023-10-07T02:20:42.136301Z",
          "shell.execute_reply.started": "2023-10-07T02:20:42.124245Z"
        },
        "id": "jNNKg5S2S2iT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class SoftmaxRegression(LinearRegression):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(SoftmaxRegression, self).__init__(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the softmax function to the linear output\n",
        "        ################# Your Implementations #################################\n",
        "        output = self.linear(x)\n",
        "        output = softmax(output)\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return output\n",
        "\n",
        "    def get_loss(self, pred_logits, targets):\n",
        "        # Calculate the cross-entropy loss\n",
        "        ################# Your Implementations #################################\n",
        "        loss = nll_loss(pred_logits, targets)\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        # Evaluate the performance of the linear regression model on the dataset\n",
        "        # x is the input data, y is the ground-truth labels\n",
        "        # Calculate the predicted labels\n",
        "        y_pred = self.forward(x)\n",
        "        y_pred = y_pred.argmax(dim=1) #changed from dim=0 to dim=1\n",
        "        return (y_pred == y).float().mean().item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:20:42.138794Z",
          "iopub.status.busy": "2023-10-07T02:20:42.137943Z",
          "iopub.status.idle": "2023-10-07T02:20:42.156256Z",
          "shell.execute_reply": "2023-10-07T02:20:42.155352Z",
          "shell.execute_reply.started": "2023-10-07T02:20:42.138765Z"
        },
        "id": "epejCR31S2iT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Prepare multi-class dataset\n",
        "# Here \"ori\" indicates \"original\"\n",
        "X_train_ori_multi, y_train_multi = multi_train.dataset['data'], multi_train.dataset['label']\n",
        "X_val_ori_multi, y_val_multi = multi_val.dataset['data'], multi_val.dataset['label']\n",
        "\n",
        "# Normalization\n",
        "X_train_multi = normalize(X_train_ori_multi)\n",
        "X_val_multi = normalize(X_val_ori_multi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE3jGM_zu6OG"
      },
      "source": [
        "Train! Don't worry if your accuracy is low, that's expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:20:42.158507Z",
          "iopub.status.busy": "2023-10-07T02:20:42.157548Z",
          "iopub.status.idle": "2023-10-07T02:25:01.355554Z",
          "shell.execute_reply": "2023-10-07T02:25:01.354548Z",
          "shell.execute_reply.started": "2023-10-07T02:20:42.158476Z"
        },
        "id": "350le3gES2iT",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "val_acc: 0.074: 100%|██████████| 100/100 [00:02<00:00, 42.00it/s]\n"
          ]
        }
      ],
      "source": [
        "sfm_model = SoftmaxRegression(3072, 20)\n",
        "# You can change the epochs accordingly here as well\n",
        "#sfm_val_accs = sfm_model.fit(X_train_multi.double(), y_train_multi, X_val_multi.double(), y_val_multi, 1e-4, 100000, 1000)\n",
        "sfm_val_accs = sfm_model.fit(X_train_multi.double(), y_train_multi, X_val_multi.double(), y_val_multi, 1e-4, 100, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR7YpMFwS2iT"
      },
      "source": [
        "### Comparison (OPTIONAL)\n",
        "\n",
        "In your own words, explain the differences between Linear Regression, Logistic Regression and Softmax Regression:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZAMBiB3u_1n"
      },
      "source": [
        "*TODO: Write your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6belwxbTvGFP"
      },
      "source": [
        "## Q2: Multi-Layer Perceptron (MLP)\n",
        "\n",
        "Now, we'll be using MiniPlaces, so there will be 100 different labels (instead of 20).\n",
        "\n",
        "You will use PyTorch to implement linear layers and ReLu to create a multi-layer perceptron."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:25:01.357760Z",
          "iopub.status.busy": "2023-10-07T02:25:01.357137Z",
          "iopub.status.idle": "2023-10-07T02:25:01.848591Z",
          "shell.execute_reply": "2023-10-07T02:25:01.847682Z",
          "shell.execute_reply.started": "2023-10-07T02:25:01.357727Z"
        },
        "id": "6hCbbEp9S2iU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class MiniPlaces(Dataset):\n",
        "    def __init__(self, root_dir, split, transform=None, label_dict=None):\n",
        "        assert split in ['train', 'val', 'test']\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.filenames = []\n",
        "        self.labels = []\n",
        "        self.label_dict = label_dict if label_dict is not None else {}\n",
        "        if split == \"train\" or split == \"val\":\n",
        "            with open(os.path.join(root_dir, (\"train\" if self.split == \"train\" else \"val\") + \".txt\")) as f:\n",
        "                for line in f:\n",
        "                    line = line.rstrip().split()\n",
        "                    n = int(line[0][-12:-4])\n",
        "                    if n <= 900:\n",
        "                        self.filenames.append(os.path.join(line[0]))\n",
        "                        self.labels.append(int(line[1]))\n",
        "        if label_dict is None and split == \"train\":\n",
        "            with open(os.path.join(root_dir, \"train.txt\")) as f:\n",
        "                num = -1\n",
        "                for line in f:\n",
        "                    line = line.rstrip().split()\n",
        "                    if int(line[1]) > num:\n",
        "                        num += 1\n",
        "                        self.label_dict.update({int(line[1]): line[0][8:line[0].find(\"/\", 8)]})\n",
        "        if split == \"test\":\n",
        "            self.labels = os.listdir(os.path.join(root_dir, \"images\", \"test\"))\n",
        "            self.filenames = [\"test/\" + i for i in self.labels]\n",
        "\n",
        "    def __len__(self):\n",
        "        dataset_len = len(self.labels)\n",
        "        return dataset_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(os.path.join(self.root_dir, \"images\", self.filenames[idx]))\n",
        "        if not self.transform is None:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "miniplaces_train = MiniPlaces(data_root, split='train', transform=data_transform)\n",
        "miniplaces_val = MiniPlaces(\n",
        "    data_root, split='val',\n",
        "    transform=data_transform,\n",
        "    label_dict=miniplaces_train.label_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:25:01.850319Z",
          "iopub.status.busy": "2023-10-07T02:25:01.850005Z",
          "iopub.status.idle": "2023-10-07T02:25:01.855105Z",
          "shell.execute_reply": "2023-10-07T02:25:01.854213Z",
          "shell.execute_reply.started": "2023-10-07T02:25:01.850289Z"
        },
        "id": "IALrjg2JS2iU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:25:01.856935Z",
          "iopub.status.busy": "2023-10-07T02:25:01.856181Z",
          "iopub.status.idle": "2023-10-07T02:25:01.866312Z",
          "shell.execute_reply": "2023-10-07T02:25:01.865313Z",
          "shell.execute_reply.started": "2023-10-07T02:25:01.856901Z"
        },
        "id": "HTAJh7JJS2iU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class FastMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        \"\"\"\n",
        "        Initialize an MLP classifier.\n",
        "        Input and output sizes of each layer:\n",
        "          1) fc1: input_size, hidden_size\n",
        "          2) fc2: hidden_size, hidden_size\n",
        "          3) fc3: hidden_size, num_classes\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Size of the input layer.\n",
        "            hidden_size (int): Size of the hidden layer.\n",
        "            num_classes (int): Number of classes in the output layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        self.fc3 = None\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Define the layers of the MLP\n",
        "        # Hint: The imported modules should give you a clue on which PyTorch functions to use\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the MLP classifier.\n",
        "        Using ReLU as the activation function after each layer, except for the output layer.\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, num_classes).\n",
        "        \"\"\"\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Implement the forward pass of the MLP classifier\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv1WHvTPvKxB"
      },
      "source": [
        "Then, define the training and evaluation functions to train and test the MLP classifier (You don't need to modify this part):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:25:01.868213Z",
          "iopub.status.busy": "2023-10-07T02:25:01.867684Z",
          "iopub.status.idle": "2023-10-07T02:25:01.886545Z",
          "shell.execute_reply": "2023-10-07T02:25:01.885542Z",
          "shell.execute_reply.started": "2023-10-07T02:25:01.868185Z"
        },
        "id": "Qy917OG_S2iU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):\n",
        "    # Place model on device\n",
        "    model = model.to(device)\n",
        "    best_acc = 0\n",
        "    flag = False\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        # Use tqdm to display a progress bar during training\n",
        "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}') as pbar:\n",
        "            for inputs, labels in train_loader:\n",
        "                # Move inputs and labels to device\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # Zero out gradients\n",
        "                optimizer.zero_grad()\n",
        "                # Compute the logits and loss\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, labels)\n",
        "                # Backpropagate the loss\n",
        "                loss.backward()\n",
        "                # Update the weights\n",
        "                optimizer.step()\n",
        "                # Update the progress bar\n",
        "                pbar.update(1)\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        avg_loss, accuracy = evaluate(model, val_loader, criterion, device)\n",
        "        if best_acc > accuracy:\n",
        "            if flag:\n",
        "                print(f'Validation set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}')\n",
        "                break\n",
        "            else:\n",
        "                flag = True\n",
        "        else:\n",
        "            best_acc = accuracy\n",
        "            flag = False\n",
        "        print(f'Validation set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}')\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate the classifier on the test set.\n",
        "\n",
        "    Args:\n",
        "        model: classifier to evaluate.\n",
        "        test_loader (torch.utils.data.DataLoader): Data loader for the test set.\n",
        "        criterion (callable): Loss function to use for evaluation.\n",
        "        device (torch.device): Device to use for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        float: Average loss on the test set.\n",
        "        float: Accuracy on the test set.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0.0\n",
        "        num_correct = 0\n",
        "        num_samples = 0\n",
        "\n",
        "        for inputs, labels in test_loader:\n",
        "            # Move inputs and labels to device\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Compute the logits and loss\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute the accuracy\n",
        "            _, predictions = torch.max(logits, dim=1)\n",
        "            num_correct += (predictions == labels).sum().item()\n",
        "            num_samples += len(inputs)\n",
        "\n",
        "    # Compute the average loss and accuracy\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = num_correct / num_samples\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def predict(model, test_dataloader):\n",
        "    \"\"\"\n",
        "    Evaluate the classifier on the test set.\n",
        "\n",
        "    Args:\n",
        "        model: classifier to evaluate.\n",
        "        test_loader (torch.utils.data.DataLoader): Data loader for the test set.\n",
        "        criterion (callable): Loss function to use for evaluation.\n",
        "        device (torch.device): Device to use for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        float: Average loss on the test set.\n",
        "        float: Accuracy on the test set.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    for i in test_dataloader:\n",
        "        pic = i[0]\n",
        "        lab = torch.argmax(model.to('cpu')(pic))\n",
        "        out.append(lab.item())\n",
        "\n",
        "    return out\n",
        "\n",
        "data_transform_flatten = transforms.Compose([data_transform, torch.flatten])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:25:01.888753Z",
          "iopub.status.busy": "2023-10-07T02:25:01.887870Z",
          "iopub.status.idle": "2023-10-07T02:25:02.170334Z",
          "shell.execute_reply": "2023-10-07T02:25:02.169396Z",
          "shell.execute_reply.started": "2023-10-07T02:25:01.888722Z"
        },
        "id": "lrz292JzS2iV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = FastMLP(\n",
        "    input_size=3 * 64 * 64,\n",
        "    hidden_size=1024,\n",
        "    num_classes=len(miniplaces_train.label_dict))\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.01,\n",
        "    momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform_flatten)\n",
        "val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform_flatten,\n",
        "    label_dict=train_dataset.label_dict)\n",
        "\n",
        "### !!! DON'T CHANGE NUM_WORKERS FROM 0 !!!\n",
        "### Using the loader will crash the notebook\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=64, num_workers=0, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=64, num_workers=0, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6rHQjo-vOfk"
      },
      "source": [
        "Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:25:02.172229Z",
          "iopub.status.busy": "2023-10-07T02:25:02.171708Z",
          "iopub.status.idle": "2023-10-07T02:27:32.844057Z",
          "shell.execute_reply": "2023-10-07T02:27:32.843133Z",
          "shell.execute_reply.started": "2023-10-07T02:25:02.172198Z"
        },
        "id": "z7Lkar0mS2iV",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/2:  11%|█         | 149/1407 [00:12<01:48, 11.63it/s, loss=4.35]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Use tqdm to display a progress bar during training\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total=\u001b[38;5;28mlen\u001b[39m(train_loader), desc=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Move inputs and labels to device\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mMiniPlaces.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     38\u001b[39m image = Image.open(os.path.join(\u001b[38;5;28mself\u001b[39m.root_dir, \u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.filenames[idx]))\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m label = \u001b[38;5;28mself\u001b[39m.labels[idx]\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torchvision/transforms/transforms.py:277\u001b[39m, in \u001b[36mNormalize.forward\u001b[39m\u001b[34m(self, tensor)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) -> Tensor:\n\u001b[32m    270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m \u001b[33;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torchvision/transforms/functional.py:350\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch.Tensor):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torchvision/transforms/_functional_tensor.py:928\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m std.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    927\u001b[39m     std = std.view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43msub_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m.div_(std)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2kvnzTMvQ9b"
      },
      "source": [
        "Don't worry if your accuracy is low, that's to be expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do5obzs5vSf2"
      },
      "source": [
        "## Q3: Convolutional Neural Network (CNN)\n",
        "\n",
        "This question is similar to the last, but you'll be implementing a convolutional neural network instead. You'll need to use convolutional layers, a max pooling layer, and dropout layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:27:32.846054Z",
          "iopub.status.busy": "2023-10-07T02:27:32.845706Z",
          "iopub.status.idle": "2023-10-07T02:27:32.858683Z",
          "shell.execute_reply": "2023-10-07T02:27:32.857864Z",
          "shell.execute_reply.started": "2023-10-07T02:27:32.846023Z"
        },
        "id": "LUnmzdS0S2iV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class FastConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_channels, conv_hidden_channels, conv_out_channels,\n",
        "        input_size=(64,64),\n",
        "        dropout_rate1=0.25, dropout_rate2=0.5,\n",
        "        fc_out_channels=128, num_classes=100,\n",
        "        kernel_size=3, stride=1, padding=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          input_channels (int): Number of channels in the input image.\n",
        "          conv_hidden_channels (int): Number of channels in the first convolutional layer.\n",
        "          conv_out_channels (int): Number of channels in the second convolutional layer.\n",
        "          input_size (tuple, optional): Height and width of the input image. (default: (64,64))\n",
        "          dropout_rate1, dropout_rate2 (float, optional): Dropout rate for\n",
        "              the first/second dropout layer. (default: 0.25, 0.5)\n",
        "          fc_out_channels (int, optional): Number of neurons in the first fully\n",
        "              connected layer. (default: 128)\n",
        "          num_classes (int, optional): Number of classes in the final output layer. (default: 100)\n",
        "          kernel_size, stride, padding (int, optional): Parameters of convolutional layers.\n",
        "\n",
        "        Initialize a convolutional neural network.\n",
        "        You can use Pytorch's built-in nn.Conv2d function.\n",
        "        Input and output shapes of each layer:\n",
        "        1) conv1: (batch_size, input_channels, H, W) -> (batch_size, conv_hidden_channels, H, W)\n",
        "        2) conv2: (batch_size, conv_hidden_channels, H, W) -> (batch_size, conv_out_channels, H, W)\n",
        "        3) max_pooling: (batch_size, conv_out_channels, H, W)\n",
        "        4) fc1: (batch_size, flatten_size) -> (batch_size, fc_out_channels)\n",
        "        5) fc2: (batch_size, fc_out_channels) -> (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # I've given you 'flatten_size', since I'm not expecting you to have this formula memorized :)\n",
        "        flatten_size = conv_out_channels * ((input_size[0] + 2 * padding - kernel_size) // stride + 1) * ((input_size[1] + 2 * padding - kernel_size) // stride + 1)\n",
        "        self.conv1 = None\n",
        "        self.conv2 = None\n",
        "        self.max_pooling = None\n",
        "        self.dropout1 = None\n",
        "        self.dropout2 = None\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Define the layers of the convolutional neural network\n",
        "        # replace \"None\"s with your implementations.\n",
        "        # All you need to do is to pass the input arguments to different constructors\n",
        "\n",
        "        # recompute flatten_size\n",
        "        H, W = input_size\n",
        "        H_out = ((H + 2*padding - kernel_size) // stride + 1)  \n",
        "        W_out = ((W + 2*padding - kernel_size) // stride + 1)\n",
        "        H_out //= 2\n",
        "        W_out //= 2\n",
        "        flatten_size = conv_out_channels * H_out * W_out\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=conv_hidden_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
        "        self.conv2 = nn.Conv2d(in_channels=conv_hidden_channels, out_channels=conv_out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
        "        self.max_pooling = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout1 = nn.Dropout2d(p=dropout_rate1)\n",
        "        self.dropout2 = nn.Dropout(p=dropout_rate2)\n",
        "        self.fc1 = nn.Linear(flatten_size, fc_out_channels)\n",
        "        self.fc2 = nn.Linear(fc_out_channels, num_classes)\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "\n",
        "    def forward(self, x, return_intermediate=False):\n",
        "        \"\"\"\n",
        "        Forward pass of the convolutional neural network.\n",
        "        The input tensor 'x' should pass through the following layers:\n",
        "        1) conv1: (batch_size, input_channels, H, W) -> (batch_size, conv_hidden_channels, H, W)\n",
        "        2) Apply relu.\n",
        "        3) conv2: (batch_size, conv_hidden_channels, H, W) -> (batch_size, conv_out_channels, H, W)\n",
        "        4) max_pooling: Perform max pooling on the output from conv2\n",
        "        5) dropout1: Perform dropout on the output from max_pooling\n",
        "        6) Flatten the output from dropout1\n",
        "        7) fc1: Pass through a fully connected layer\n",
        "        8) dropout2: Perform dropout on the output from fc1\n",
        "        9) Apply relu.\n",
        "        10) fc2: Pass the output from the actiction layer to through a fully connected\n",
        "                layer to produce the final output\n",
        "        \"\"\"\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Implement the forward pass of the convolutional neural network\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.max_pooling(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        ################# End of your Implementations ##########################\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:27:32.863492Z",
          "iopub.status.busy": "2023-10-07T02:27:32.863259Z",
          "iopub.status.idle": "2023-10-07T02:27:33.660811Z",
          "shell.execute_reply": "2023-10-07T02:27:33.659848Z",
          "shell.execute_reply.started": "2023-10-07T02:27:32.863472Z"
        },
        "id": "U3L6j5hPS2iW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "conv_train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform)\n",
        "conv_val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform)\n",
        "conv_train_loader = torch.utils.data.DataLoader(\n",
        "    conv_train_dataset, batch_size=64, num_workers=0, shuffle=True)\n",
        "conv_val_loader = torch.utils.data.DataLoader(\n",
        "    conv_val_dataset, batch_size=64, num_workers=0, shuffle=False)\n",
        "\n",
        "model = FastConv(\n",
        "    input_channels=3, conv_hidden_channels=64, conv_out_channels=128,\n",
        "    input_size=(64,64),\n",
        "    dropout_rate1=0.25, dropout_rate2=0.5,\n",
        "    fc_out_channels=128,\n",
        "    kernel_size=3, stride=1, padding=1,\n",
        "    num_classes=len(miniplaces_train.label_dict))\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0m77a-xvXv3"
      },
      "source": [
        "Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:27:33.662588Z",
          "iopub.status.busy": "2023-10-07T02:27:33.662236Z",
          "iopub.status.idle": "2023-10-07T02:31:35.175404Z",
          "shell.execute_reply": "2023-10-07T02:31:35.174516Z",
          "shell.execute_reply.started": "2023-10-07T02:27:33.662556Z"
        },
        "id": "dFq88hc5S2iW",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/2:   2%|▏         | 34/1407 [00:03<02:32,  9.03it/s, loss=4.55]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_val_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Use tqdm to display a progress bar during training\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total=\u001b[38;5;28mlen\u001b[39m(train_loader), desc=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Move inputs and labels to device\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mMiniPlaces.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     38\u001b[39m image = Image.open(os.path.join(\u001b[38;5;28mself\u001b[39m.root_dir, \u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.filenames[idx]))\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m label = \u001b[38;5;28mself\u001b[39m.labels[idx]\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torchvision/transforms/functional.py:168\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[32m    167\u001b[39m mode_to_nptype = {\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m: np.int32, \u001b[33m\"\u001b[39m\u001b[33mI;16\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.byteorder == \u001b[33m\"\u001b[39m\u001b[33mlittle\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mI;16B\u001b[39m\u001b[33m\"\u001b[39m: np.int16, \u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m: np.float32}\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m img = torch.from_numpy(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pic.mode == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    171\u001b[39m     img = \u001b[32m255\u001b[39m * img\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/PIL/Image.py:726\u001b[39m, in \u001b[36mImage.__array_interface__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    720\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"iPython display hook support for JPEG format.\u001b[39;00m\n\u001b[32m    721\u001b[39m \n\u001b[32m    722\u001b[39m \u001b[33;03m    :returns: JPEG version of the image as bytes\u001b[39;00m\n\u001b[32m    723\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    724\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._repr_image(\u001b[33m\"\u001b[39m\u001b[33mJPEG\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__array_interface__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28mbytes\u001b[39m | \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, ...]]:\n\u001b[32m    728\u001b[39m     \u001b[38;5;66;03m# numpy array interface support\u001b[39;00m\n\u001b[32m    729\u001b[39m     new: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28mbytes\u001b[39m | \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, ...]] = {\u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m}\n\u001b[32m    730\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    731\u001b[39m         \u001b[38;5;66;03m# Binary images need to be extended from bits to bytes\u001b[39;00m\n\u001b[32m    732\u001b[39m         \u001b[38;5;66;03m# See: https://github.com/python-pillow/Pillow/issues/350\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "train(model, conv_train_loader, conv_val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myZY1O24vYp5"
      },
      "source": [
        "I can get ~20% accuracy after two epochs. How about you?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96dwsv_9vb5j"
      },
      "source": [
        "## Q4: Your own model\n",
        "\n",
        "You will construct your own model using built-in convolutional layers. You can base your model on the provided `FastConv` class, or choose to modify the number of convolutional layers, feature size, learning rate, optimizer, and other parameters to suit your needs. You can also use any data transformations you'd like. You can even base your model off of an already existing architecture. The only restriction is that you can't blindly import an entire model at once; you should be creating every layer using PyTorch. Create your model and train using cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:31:35.177651Z",
          "iopub.status.busy": "2023-10-07T02:31:35.176994Z",
          "iopub.status.idle": "2023-10-07T02:31:35.192199Z",
          "shell.execute_reply": "2023-10-07T02:31:35.191285Z",
          "shell.execute_reply.started": "2023-10-07T02:31:35.177602Z"
        },
        "id": "2jnrWmTJS2iW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Use these cells to design and train your model\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_channels, conv_hidden_channels, conv_out_channels,\n",
        "        input_size=(64,64),\n",
        "        dropout_rate1=0.25, dropout_rate2=0.5,\n",
        "        fc_out_channels=128, num_classes=100,\n",
        "        kernel_size=3, stride=1, padding=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          input_channels (int): Number of channels in the input image.\n",
        "          conv_hidden_channels (int): Number of channels in the first convolutional layer.\n",
        "          conv_out_channels (int): Number of channels in the second convolutional layer.\n",
        "          input_size (tuple, optional): Height and width of the input image. (default: (64,64))\n",
        "          dropout_rate1, dropout_rate2 (float, optional): Dropout rate for\n",
        "              the first/second dropout layer. (default: 0.25, 0.5)\n",
        "          fc_out_channels (int, optional): Number of neurons in the first fully\n",
        "              connected layer. (default: 128)\n",
        "          num_classes (int, optional): Number of classes in the final output layer. (default: 100)\n",
        "          kernel_size, stride, padding (int, optional): Parameters of convolutional layers.\n",
        "\n",
        "        Initialize a convolutional neural network.\n",
        "        You can use Pytorch's built-in nn.Conv2d function.\n",
        "        Input and output shapes of each layer:\n",
        "        1) conv1: (batch_size, input_channels, H, W) -> (batch_size, conv_hidden_channels, H, W)\n",
        "        2) conv2: (batch_size, conv_hidden_channels, H, W) -> (batch_size, conv_out_channels, H, W)\n",
        "        3) max_pooling: (batch_size, conv_out_channels, H, W)\n",
        "        4) fc1: (batch_size, flatten_size) -> (batch_size, fc_out_channels)\n",
        "        5) fc2: (batch_size, fc_out_channels) -> (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        H, W = input_size\n",
        "        H = (H + 2*padding - kernel_size)//stride + 1\n",
        "        W = (W + 2*padding - kernel_size)//stride + 1\n",
        "        H = (H + 2*padding - kernel_size)//stride + 1\n",
        "        W = (W + 2*padding - kernel_size)//stride + 1\n",
        "        H //= 2\n",
        "        W //= 2\n",
        "        H = (H + 2*padding - kernel_size)//stride + 1\n",
        "        W = (W + 2*padding - kernel_size)//stride + 1\n",
        "        H //= 2\n",
        "        W //= 2\n",
        "\n",
        "        flatten_size = (conv_out_channels * 2) * H * W\n",
        "\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=conv_hidden_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
        "        self.conv2 = nn.Conv2d(in_channels=conv_hidden_channels, out_channels=conv_out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
        "        self.conv3 = nn.Conv2d(conv_out_channels, conv_out_channels*2, kernel_size, stride, padding)\n",
        "        self.max_pooling = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout1 = nn.Dropout2d(p=dropout_rate1)\n",
        "        self.dropout2 = nn.Dropout(p=dropout_rate2)\n",
        "        self.fc1 = nn.Linear(flatten_size, fc_out_channels)\n",
        "        self.fc2 = nn.Linear(fc_out_channels, num_classes)\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "\n",
        "    def forward(self, x, return_intermediate=False):\n",
        "        \"\"\"\n",
        "        Forward pass of the convolutional neural network.\n",
        "        The input tensor 'x' should pass through the following layers:\n",
        "        1) conv1: (batch_size, input_channels, H, W) -> (batch_size, conv_hidden_channels, H, W)\n",
        "        2) Apply relu.\n",
        "        3) conv2: (batch_size, conv_hidden_channels, H, W) -> (batch_size, conv_out_channels, H, W)\n",
        "        4) max_pooling: Perform max pooling on the output from conv2\n",
        "        5) dropout1: Perform dropout on the output from max_pooling\n",
        "        6) Flatten the output from dropout1\n",
        "        7) fc1: Pass through a fully connected layer\n",
        "        8) dropout2: Perform dropout on the output from fc1\n",
        "        9) Apply relu.\n",
        "        10) fc2: Pass the output from the actiction layer to through a fully connected\n",
        "                layer to produce the final output\n",
        "        \"\"\"\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Implement the forward pass of the convolutional neural network\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.max_pooling(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.max_pooling(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T02:31:35.194237Z",
          "iopub.status.busy": "2023-10-07T02:31:35.193796Z",
          "iopub.status.idle": "2023-10-07T02:31:35.912492Z",
          "shell.execute_reply": "2023-10-07T02:31:35.911583Z",
          "shell.execute_reply.started": "2023-10-07T02:31:35.194205Z"
        },
        "id": "zbwdIV_dS2iW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Previous questions will be super useful"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRu2l4A5vj7_"
      },
      "source": [
        "Now that you've designed your model and trained it, it's time to test it's accuracy! Use the \"test\" images and the 'predict' function.\n",
        "\n",
        "NOTE: Make sure when constructing your DataLoader, you set 'shuffle' to 'False'. This is so I can match your model's predictions to test images to find how accurate its predictions are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yME96Ltrv4hN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/2:   3%|▎         | 44/1407 [00:05<03:02,  7.48it/s, loss=4.62]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m0.01\u001b[39m)\n\u001b[32m     21\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_val_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\u001b[39m\n\u001b[32m     23\u001b[39m         \u001b[38;5;66;03m# Update the progress bar\u001b[39;00m\n\u001b[32m     24\u001b[39m         pbar.update(\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m         pbar.set_postfix(loss=\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Evaluate the model on the validation set\u001b[39;00m\n\u001b[32m     28\u001b[39m avg_loss, accuracy = evaluate(model, val_loader, criterion, device)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "### Create your DataLoader and use predict to get your model's predictions\n",
        "conv_train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform)\n",
        "conv_val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform)\n",
        "conv_train_loader = torch.utils.data.DataLoader(\n",
        "    conv_train_dataset, batch_size=64, num_workers=0, shuffle=True)\n",
        "conv_val_loader = torch.utils.data.DataLoader(\n",
        "    conv_val_dataset, batch_size=64, num_workers=0, shuffle=False)\n",
        "\n",
        "model = MyModel(\n",
        "    input_channels=3, conv_hidden_channels=64, conv_out_channels=128,\n",
        "    input_size=(64,64),\n",
        "    dropout_rate1=0.25, dropout_rate2=0.5,\n",
        "    fc_out_channels=128,\n",
        "    kernel_size=3, stride=1, padding=1,\n",
        "    num_classes=len(miniplaces_train.label_dict))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train(model, conv_train_loader, conv_val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "test_preds = predict(model,  conv_val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kgHXoee2fQL"
      },
      "source": [
        "Now that you have a list containing your predictions, you should create a file named \"{your_name}.json\", containing \"{id: predicted_label}\" pairs. A function to do this has been provided, just pass in the output of the predict function from the previous cell. You can find the file under the \"data\" folder by clicking on the folder icon in Colab, or in your working directory output in Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-07T03:14:31.910276Z",
          "iopub.status.busy": "2023-10-07T03:14:31.909938Z",
          "iopub.status.idle": "2023-10-07T03:14:31.917337Z",
          "shell.execute_reply": "2023-10-07T03:14:31.916417Z",
          "shell.execute_reply.started": "2023-10-07T03:14:31.910247Z"
        },
        "id": "XpAuKsroS2iX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def preds_to_json(preds):\n",
        "    json_dict = {}\n",
        "    for i, n in enumerate(preds):\n",
        "        json_dict[str('0000000'+str(i+1))[-8:]] = n\n",
        "    with open(os.path.join(root_dir,'your_name.json'), 'w', encoding='utf-8') as f:\n",
        "        #If you're using a Kaggle notebook, change root_dir to '/kaggle/working/'\n",
        "        json.dump(json_dict, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "## 'test_preds' should be the output of 'predict' from the previous cell\n",
        "preds_to_json(test_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYPqu_8qwdc5"
      },
      "source": [
        "### Model Architecture Explanation\n",
        "\n",
        "In the cell below, explain your model's architecture and how you came up with it. Feel free to include as many details as you want, including how you decided on a learning rate or data transformations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NPL4ZE5wfc3"
      },
      "source": [
        "*TODO: Write your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jFcq5U_wh1R"
      },
      "source": [
        "## You've Finished!!!\n",
        "\n",
        "Great job completing the assessment! The root directory for this notebook should now include the .json with your test predictions. In order to submit this assessment, all you have to do is email this notebook as an .ipynb and the .json to yuliaanashkina@g.ucla.edu AND danielavartani@g.ucla.edu\n",
        "\n",
        "Thanks for taking the time to work through these problems! We look forward to reading through your application. You should expect to hear back a few days after April 6th."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
